import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from time import time

import tensorflow as tf
import tensorflow.keras.backend as K

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import *
from tensorflow.keras import layers
from tensorflow.keras.callbacks import History, EarlyStopping
from tensorflow import keras


dic_country = \
{'AUS': 'Australia',
 'BRA': 'Brazil',
 'CAN': 'India',
 'CHN': 'China',
 'GBR': 'United Kingdom',
 'IND': 'India',
 'JPN': 'Japan',
 'SGP': 'Singapore',
 'USA': 'United States'}
pd.DataFrame(dic_country,index=[0]).T.reset_index().rename(columns = {'index': 'country_code', 0: 'country_name'})


Data_dic_m = pd.ExcelFile('../../Data/cleaned/Manufacturing_filled.xlsx')
Data_dic_s = pd.ExcelFile('../../Data/cleaned/Service_filled.xlsx')


def get_sheet_by_methods(data, method_num, verbose = 0):
  sheet_names = data.sheet_names
  sheet_imputation_map = pd.DataFrame([[s, s[:3].strip(),s[3:]] for s in sheet_names], columns = ['sheet_name', 'country_code', 'imputation method'])
  methods = sheet_imputation_map['imputation method'].unique()
  if verbose:
    print('methods tried:',methods, len(methods))
  return list(sheet_imputation_map.loc[sheet_imputation_map['imputation method'] == methods[method_num]]['sheet_name'].values)


m_dfs = {}
for i in range(len(Data_dic_m.sheet_names) // 10):
  for sheet in get_sheet_by_methods(Data_dic_m, i):
    df = pd.read_excel(Data_dic_m, sheet_name=sheet)
    df['Year'] = [i.year for i in pd.to_datetime(df.Year, format='%Y')]
    df = df.set_index('Year')
    m_dfs[(sheet[:3], sheet[3:].strip())] = df


s_dfs = {}
for i in range(len(Data_dic_s.sheet_names) // 10):
  for sheet in get_sheet_by_methods(Data_dic_s, i):
    df = pd.read_excel(Data_dic_s, sheet_name=sheet)
    df['Year'] = [i.year for i in pd.to_datetime(df.Year, format='%Y')]
    df = df.set_index('Year')
    s_dfs[(sheet[:3], sheet[3:].strip())] = df


class MRobustScaler():
      """
      Scale features using statistics that are robust to outliers.
      """
      def __init__(self, q1=0.2, q2=0.8):
          self.q1 = q1
          self.q2 = q2

      def fit(self, df, eps):
          """
          Return quantile range and median of all features

          Set qrange be 1 if feature has very small range, otherwise calculate
          the quantile range with specified quantile
          """
          df = df.astype('float64') # convert object to float
          self.df_median = df.median() #pd median uses np.nanmedian
          df_q1 = df.quantile(self.q1)
          df_q2 = df.quantile(self.q2)
          qrange = np.array(df_q2 - df_q1) #Near constant features have a very small range
          if np.isscalar(qrange):
                if qrange < eps:
                      qrange = 1.0
          elif isinstance(qrange, np.ndarray):
                constant_mask = qrange < np.ones_like(qrange)*eps
                qrange[constant_mask] = 1.0
          self.qrange = qrange
          self.df_qrange = pd.Series(index = self.df_median.index, data=qrange)
          return self.qrange, self.df_median
      
      def get_fitted(self):
        return self.df_median, self.qrange

      def transform(self, df, eps = 0.1):
          qrange, df_median = self.fit(df, eps = eps)
          return (df - df_median)/qrange


def train_test_split(df, train_ratio = 0.7, val_ratio = 0.9, IW = 3):
  """
  By default: train:val:test = 0.7:0.2:0.1
  """
  n = len(df)
  train_df = df[0:int(n*train_ratio)]
  val_df = df[(int(n*train_ratio)-IW):int(n*val_ratio)]
  test_df = df[(int(n*val_ratio)-IW):]
  return train_df, val_df, test_df


class WindowGenerator():
  def __init__(self, input_width, label_width, shift,
               train_df, val_df, test_df, scaled_data,
               label_columns=None):
    # Store the raw data.
    self.train_df = train_df
    self.val_df = val_df
    self.test_df = test_df
    self.scaled_data = scaled_data

    # Work out the label column indices.
    self.label_columns = label_columns
    if label_columns is not None:
      self.label_columns_indices = {name: i for i, name in
                                    enumerate(label_columns)}
    self.column_indices = {name: i for i, name in
                           enumerate(train_df.columns)}

    # Work out the window parameters.
    self.input_width = input_width
    self.label_width = label_width
    self.shift = shift

    self.total_window_size = input_width + shift

    self.input_slice = slice(0, input_width)
    self.input_indices = np.arange(self.total_window_size)[self.input_slice]

    self.label_start = self.total_window_size - self.label_width
    self.labels_slice = slice(self.label_start, None)
    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

  def __repr__(self):
    return '\n'.join([
        f'Total window size: {self.total_window_size}',
        f'Input indices: {self.input_indices}',
        f'Label indices: {self.label_indices}',
        f'Label column name(s): {self.label_columns}'])
  
  def split_window(self, features):
    inputs = features[:, self.input_slice, :]
    labels = features[:, self.labels_slice, :]
    if self.label_columns is not None:
      labels = tf.stack(
          [labels[:, :, self.column_indices[name]] for name in self.label_columns],
          axis=-1)

    inputs.set_shape([None, self.input_width, None])
    labels.set_shape([None, self.label_width, None])
    return inputs, labels
  
  def make_dataset(self, data):
    data = np.array(data, dtype=np.float32)
    ds = tf.keras.utils.timeseries_dataset_from_array(
        data=data,
        targets=None,
        sequence_length=self.total_window_size,
        sequence_stride=1,
        shuffle=False,
        batch_size=32,)
    ds = ds.map(self.split_window)
    return ds
  
  @property
  def train(self):
    return self.make_dataset(self.train_df)

  @property
  def val(self):
    return self.make_dataset(self.val_df)

  @property
  def test(self):
    return self.make_dataset(self.test_df)
  
  @property
  def all_data(self):
    return self.make_dataset(self.scaled_data)


class LSTM_base:

  def __init__(self, lookback_width, num_features, loss=None, layer1=512, layer2=256, layer3=256, label_width=1):
    self.lookback_width = lookback_width
    self.num_features = num_features
    self.loss = loss 
    self.layer1 = layer1
    self.layer2 = layer2
    self.layer3 = layer3
    self.label_width = label_width
    self.create_model()
    self.history = None

  def create_model(self):
    input = layers.Input(shape=(self.lookback_width, self.num_features), name='input')
    x = layers.LSTM(self.layer1, return_sequences=True)(input)
    x = layers.Dropout(0.05)(x)
    x = layers.LSTM(self.layer2, return_sequences=True)(x)
    x = layers.Dropout(0.1)(x)
    x = layers.LSTM(self.layer3)(x)
    x = layers.Dropout(0.1)(x)
    output = layers.Dense(self.label_width)(x)
    model = keras.Model(input, outputs=output)
    self.model = model

  def compile_and_fit(self, data_window, epochs, batch_size, verbose, learning_rate, patience=4):
    loss=self.loss,
    history = History()
    batch_size = batch_size[0] if isinstance(batch_size, tuple) else batch_size
    self.model.compile(loss=loss,
                       optimizer=keras.optimizers.Adam(learning_rate=learning_rate))
    self.history = self.model.fit(data_window.train,
                                  epochs=epochs,
                                  batch_size=batch_size,
                                  validation_data=data_window.val, 
                                  callbacks=[history, EarlyStopping(monitor='val_loss',
                                                           patience=patience,
                                                           mode='min',
                                                           restore_best_weights=True)],
                                  verbose=verbose,
                                  shuffle=False)
    return self.history

  def plot(self):
    display(tf.keras.utils.plot_model(self.model, show_shapes=True))
  
  def train(self, data_window, MAX_EPOCHS, bs, verbose, lr, patience):
    start = time()
    self.data_window = data_window
    self.compile_and_fit(data_window=data_window,
                                epochs=MAX_EPOCHS,
                                batch_size=bs,
                                verbose=verbose,
                                learning_rate=lr,
                                patience=patience)
    delta=time()-start
    self.delta = delta
    print('Total Modelling Time: ',delta)


def fetch_pred_real(rbscaler, model):
  med, qrg = rbscaler.get_fitted()
  target_median, qrange = med.values[-1], qrg[-1]
  y_pred = model.model.predict(model.data_window.test)
  y_pred = np.array(y_pred)*qrange+target_median
  y_real = np.array([y*qrange+target_median for _, y in model.data_window.test]).reshape(y_pred.shape)
  return y_pred, y_real


def MAE(y_pred, y_real, verbose = 1):
  E = y_pred-y_real
  return np.mean(np.abs(E), axis = 0), np.mean(np.abs(E))

def MSE(y_pred, y_real):
  E = y_pred-y_real
  return np.mean(np.square(E), axis = 0), np.mean(np.square(E))

def RMSE(y_pred, y_real):
  np.sqrt(MSE(y_pred, y_real)[0])
  return np.sqrt(MSE(y_pred, y_real)[0]), np.sqrt(MSE(y_pred, y_real)[1])


def fetch_all_pred_real(rbscaler, model):
  med, qrg = rbscaler.get_fitted()
  target_median, qrange = med.values[-1], qrg[-1]
  y_pred = model.model.predict(model.data_window.all_data)
  y_pred = np.array(y_pred)*qrange+target_median
  y = None
  for _, j in  model.data_window.all_data:
    if y is None:
      y = j*qrange+target_median
    else:
      y = np.concatenate((y, j*qrange+target_median))
  y_real = y.reshape(y_pred.shape)
  return y_pred, y_real


def pat_pred(y, data):
  """
  y could be y_real or y_pred
  """
  d = pd.DataFrame({})
  for i in range(y.shape[1]):
    d[f'gap_{i}'] =  [np.nan] * i + list(y[:, i]) + [np.nan] * (y.shape[1]-i-1)
  d.index = data[IW:].index
  return d


def forecast(rbscaler, scaled_data, model, name = 'Manufacturing_value added_%_of_GDP'):
  med, qrg = rbscaler.get_fitted()
  target_median, qrange = med.values[-1], qrg[-1]
  y = model.model.predict(np.expand_dims(scaled_data.iloc[-3:, :], axis = 0))
  df =  pd.DataFrame({name: (np.array(y)*qrange+target_median).tolist()[0]}, index =  [scaled_data.index[-1]+i for i in range(1, 6)])
  return df


SHIFT = 5
LW = 5
IW = 3
MAX_EPOCHS = 50
bs = 32
verbose = True
lr = 0.001
patience = 50
LOSS = tf.keras.losses.MeanAbsoluteError()


data = m_dfs[('CHN', 'Median')]
rbscaler = MRobustScaler() 
scaled_data = rbscaler.transform(data, 0.1)


train_df, val_df, test_df=train_test_split(scaled_data, IW = IW + SHIFT)
# use 4 year to predict 1 year ahead
wg = WindowGenerator(input_width=IW, label_width=LW, shift=SHIFT,
                     train_df = train_df, val_df = val_df, test_df = test_df, 
                     scaled_data = scaled_data,
                     label_columns=[scaled_data.columns[-1]])
wg


model = LSTM_base(lookback_width=IW, num_features = len(scaled_data.columns),label_width=LW, loss = LOSS)
model.plot()
# !brew install graphviz # for mac
# !pip install pydot


model.train(data_window=wg, MAX_EPOCHS = MAX_EPOCHS, bs=bs, verbose=verbose, lr=lr, patience = patience)


y_pred, y_real = fetch_pred_real(rbscaler, model)
print(
'MAE', list(MAE(y_pred, y_real)[0]), ' Mean MAE', MAE(y_pred, y_real)[1],
'\nMSE', list(MSE(y_pred, y_real)[0]),' Mean MSE', MSE(y_pred, y_real)[1],
'\nRMSE', list(RMSE(y_pred, y_real)[0]), 'Mean RMSE', RMSE(y_pred, y_real)[1]
)


pat_pred


y_pred, y_real = fetch_all_pred_real(rbscaler, model)
### predict 5 years at once (gap as prediction gap : 0, 1, 2, 3, 4)
pd.concat([data.iloc[:, -1], pat_pred(y_pred)], axis = 1).plot(figsize = (20, 4), title = 'China')


forecast(rbscaler, model)


SHIFT = 5
LW = 5
IW = 3
MAX_EPOCHS = 50
bs = 32
verbose = False
lr = 0.001
patience = 50
LOSS = tf.keras.losses.MeanAbsoluteError()


def pipeline(country = 'CHN', method = 'Median'):
  data = m_dfs[(country, method)].dropna(axis = 1)
  rbscaler = MRobustScaler() 
  scaled_data = rbscaler.transform(data, 0.1)
  train_df, val_df, test_df=train_test_split(scaled_data, IW = IW + SHIFT)
  wg = WindowGenerator(input_width=IW, label_width=LW, shift=SHIFT,
                      train_df = train_df, val_df = val_df, test_df = test_df, 
                      scaled_data = scaled_data,
                      label_columns=[scaled_data.columns[-1]])
  model = LSTM_base(lookback_width=IW, num_features = len(scaled_data.columns),label_width=LW, loss = LOSS)
  model.train(data_window=wg, MAX_EPOCHS = MAX_EPOCHS, bs=bs, verbose=verbose, lr=lr, patience = patience)
  y_pred, y_real = fetch_pred_real(rbscaler, model)
  print(
  'MAE', list(MAE(y_pred, y_real)[0]), ' Mean MAE', MAE(y_pred, y_real)[1],
  '\nMSE', list(MSE(y_pred, y_real)[0]),' Mean MSE', MSE(y_pred, y_real)[1],
  '\nRMSE', list(RMSE(y_pred, y_real)[0]), 'Mean RMSE', RMSE(y_pred, y_real)[1]
  )
  y_pred1, y_real1 = fetch_all_pred_real(rbscaler, model)
  ### predict 5 years at once (gap as prediction gap : 0, 1, 2, 3, 4)
  display(pd.concat([data.iloc[:, -1], pat_pred(y_pred1, data)], axis = 1).plot(figsize = (20, 4), title = dic_country[country] + ' (' +  method + ') :' \
                                                                         + '\nMean MAE: ' + str(MAE(y_pred, y_real)[1]) \
                                                                         + '\nMean MSE: ' + str(MSE(y_pred, y_real)[1]) \
                                                                         + '\nMean RMSE: ' + str(RMSE(y_pred, y_real)[1]) \
                                                                         ))
  display(forecast(rbscaler, scaled_data, model, name = country + '_' + method))
  return  MAE(y_pred, y_real)[1], MSE(y_pred, y_real)[1], RMSE(y_pred, y_real)[1]


pipeline()


c, m, a, s, r = [], [], [], [], []


for sheet in get_sheet_by_methods(Data_dic_m, 0):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  a1, s1, r1 = pipeline(country, method)
  c.append(country)
  m.append(method)
  a.append(a1)
  s.append(s1)
  r.append(r1)
  print('\n')


for sheet in get_sheet_by_methods(Data_dic_m, 1):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  a1, s1, r1 = pipeline(country, method)
  c.append(country)
  m.append(method)
  a.append(a1)
  s.append(s1)
  r.append(r1)
  print('\n')


for sheet in get_sheet_by_methods(Data_dic_m, 2):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  try:
    a1, s1, r1 = pipeline(country, method)
    c.append(country)
    m.append(method)
    a.append(a1)
    s.append(s1)
    r.append(r1)
    print('\n')
  except:
    print(f'Cannot handle {country}, {method}')


for sheet in get_sheet_by_methods(Data_dic_m, 3):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  try:
    a1, s1, r1 = pipeline(country, method)
    c.append(country)
    m.append(method)
    a.append(a1)
    s.append(s1)
    r.append(r1)
    print('\n')
  except:
    print(f'Cannot handle {country}, {method}')


for sheet in get_sheet_by_methods(Data_dic_m, 4):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  try:
    a1, s1, r1 = pipeline(country, method)
    c.append(country)
    m.append(method)
    a.append(a1)
    s.append(s1)
    r.append(r1)
    print('\n')
  except:
    print(f'Cannot handle {country}, {method}')


for sheet in get_sheet_by_methods(Data_dic_m, 5):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  try:
    a1, s1, r1 = pipeline(country, method)
    c.append(country)
    m.append(method)
    a.append(a1)
    s.append(s1)
    r.append(r1)
    print('\n')
  except:
    print(f'Cannot handle {country}, {method}')


pd.DataFrame({
    'country': c,
    'method': m,
    'MAE': a,
    'MSE': s,
    'RMSE': r
})


c, m, a, s, r = [], [], [], [], []


for sheet in get_sheet_by_methods(Data_dic_s, 0):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  a1, s1, r1 = pipeline(country, method)
  c.append(country)
  m.append(method)
  a.append(a1)
  s.append(s1)
  r.append(r1)
  print('\n')


for sheet in get_sheet_by_methods(Data_dic_s, 1):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  try:
    a1, s1, r1 = pipeline(country, method)
    c.append(country)
    m.append(method)
    a.append(a1)
    s.append(s1)
    r.append(r1)
    print('\n')
  except:
    print(f'Cannot handle {country}, {method}')


for sheet in get_sheet_by_methods(Data_dic_s, 2):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  try:
    a1, s1, r1 = pipeline(country, method)
    c.append(country)
    m.append(method)
    a.append(a1)
    s.append(s1)
    r.append(r1)
    print('\n')
  except:
    print(f'Cannot handle {country}, {method}')


for sheet in get_sheet_by_methods(Data_dic_s, 3):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  try:
    a1, s1, r1 = pipeline(country, method)
    c.append(country)
    m.append(method)
    a.append(a1)
    s.append(s1)
    r.append(r1)
    print('\n')
  except:
    print(f'Cannot handle {country}, {method}')


for sheet in get_sheet_by_methods(Data_dic_s, 4):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  try:
    a1, s1, r1 = pipeline(country, method)
    c.append(country)
    m.append(method)
    a.append(a1)
    s.append(s1)
    r.append(r1)
    print('\n')
  except:
    print(f'Cannot handle {country}, {method}')


for sheet in get_sheet_by_methods(Data_dic_s, 5):
  country = sheet[:3]
  method = sheet[3:].strip()
  print(dic_country[country], method, ':')
  try:
    a1, s1, r1 = pipeline(country, method)
    c.append(country)
    m.append(method)
    a.append(a1)
    s.append(s1)
    r.append(r1)
    print('\n')
  except:
    print(f'Cannot handle {country}, {method}')


pd.DataFrame({
    'country': c,
    'method': m,
    'ME': a,
    'MSE': s,
    'RMSE': r
})
